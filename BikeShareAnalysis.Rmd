---
title: "Bike Sharing Program: Rental Demand Prediction from Weather"
author: "MattDSquared (Matthew D. Davis)"
date: "Monday, May 25, 2015"
output: 
    html_document:
        keep_md: true
---

_From the competition host, Kaggle.com_

Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.

The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city. In this competition, participants are asked to combine historical usage patterns with weather data in order to forecast bike rental demand in the Capital Bikeshare program in Washington, D.C.

###Acknowledgements

Kaggle is hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:

Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.

## Project Setup

```{r setup, warning=FALSE, message=FALSE}
library(ggplot2)
library(ggdendro)
library(lubridate)
library(plyr); library(dplyr)
library(randomForest)
library(scales)

sessionInfo()

setwd("~/../datascience/BikeSharingDemand")
graphics.off()
windows.options(width = 12, height = 8, xpinch=96, ypinch=96, xpos=0, ypos=0)
```

## Data Processing

Data is made available at https://www.kaggle.com/c/bike-sharing-demand/data. A Kaggle account is required. This needs to be downloaded manually (it's behind a login, which just complicates automated download). Alternatively, forking the BikeSharingDemand repo at https://github.com/MattDSquared/BikeSharingDemand will include this data set. 

```{r download, warning=FALSE}
dir.create("data")
train.filepath <- "data/train.csv"
test.filepath <- "data/test.csv"

# download train and test data sets from 
# https://www.kaggle.com/c/bike-sharing-demand/data
# place train.csv and test.csv into data subdirectory
# data obtained 2015-05-25
```

Load the data into train and test data sets.

```{r load}
train <- read.csv(train.filepath, stringsAsFactors=FALSE)
test <- read.csv(test.filepath, stringsAsFactors=FALSE)
```

Text data is cleaned up and simplified. The weather labels are a bit verbose, see the [competition data](https://www.kaggle.com/c/bike-sharing-demand/data) page for formal weather descriptions.

```{r clean.factorlabels}
weatherlabels <- c("nice weather", "cloudy/misty", "light weather",
                   "heavy weather")
seasons <- c("spring", "summer", "fall", "winter")
daysoftheweek <- c("Monday","Tuesday","Wednesday","Thursday","Friday",
                   "Saturday","Sunday")
train <- mutate(train, 
                datetime = ymd_hms(datetime),
                season = factor(season, levels=1:4, labels=seasons),
                holiday = as.logical(holiday),
                workingday = factor(workingday, levels=c(1,0), 
                                    labels=c("Workday","Holiday/Weekend")),
                weather = factor(weather,levels=1:4, labels=weatherlabels),
                dayofweek = factor(weekdays(datetime), levels=daysoftheweek),
                timeofday = hour(datetime))
test <- mutate(test, 
                datetime = ymd_hms(datetime),
                season = factor(season, levels=1:4, labels=seasons),
                holiday = as.logical(holiday),
                workingday = factor(workingday, levels=c(1,0), 
                                    labels=c("Workday","Holiday/Weekend")),
                weather = factor(weather,levels=1:4, labels=weatherlabels),
                dayofweek = factor(weekdays(datetime), levels=daysoftheweek),
                timeofday = hour(datetime))

```

The data only show a single heavy weather point in the midst of several light weather points. For graphing purposes, replace with light weather. Implicit assumption: bike rentals during heavy weather will be negligible. 

```{r clean.heavyweather}
print(which(train$weather == "heavy weather"))
print(which(test$weather == "heavy weather"))
print(train[which(train$weather == "heavy weather")+(-5):6,c("datetime","weather", "temp", "count")])

## interpolate heavy weather as light weather for graphing purposes
train$weather[train$weather == "heavy weather"] = "light weather"
test$weather[test$weather == "heavy weather"] = "light weather"
```

This should be explored further. Is this data set applicable to the DC area, since it essentially doesn't include heavy weather data which does occur in the DC area. 

## What is atemp?

The atemp variable in the data set is the "feels like" temperature based on weather factors. The plot below illustrates this relationship.

```{r atemp.plot}
## atemp vs. temperature, weather
gg <- ggplot(train, aes(temp, atemp, color=humidity, size=windspeed)) +
    geom_point() +
    scale_colour_gradient(low="red",high="blue") + 
    labs(title="Effects on 'Feels Like' Tempurature") +
    labs(x="tempurature (°C)") +
    labs(y="'feels like' tempurature (°C)")
windows()
print(gg)
```

This shows the effects of windchill below about 15°C and humidity above about 24°C. Note the data contains few points below 5°C, which could be an issue for the DC area. 

Also note the outliers to the lower right of the main grouping. These `r nrow(filter(train, atemp < 15, temp > 24))` data all occur on a single day in august.
Note the fixed atemp value of `r filter(train, atemp < 15, temp > 24)$atemp[1]`. For simplicity, assume for these values atemp = temp.

```{r atemp.outliers}
select(filter(train, atemp < 15, temp > 24), datetime, weather, temp, atemp, humidity, windspeed)

# replace atemp with temp for outliers
train <- mutate(train, atemp=ifelse((atemp < 15) & (temp > 24), temp, atemp))
test <- mutate(test, atemp=ifelse((atemp < 15) & (temp > 24), temp, atemp))
```

It appears wind and humidity are already packaged into the 'feels like' temperature value. The key is to find out how much that matters. 

## Primary demand drivers

### principal component analysis (PCA)

A principal component analysis (PCA) on the data shows how variation in the input data can be explained by a same-sized set of orthogonal variables. In other words, it indicates where most of the information is contained in a data set. 

```{r pca}
tr.inputs <- select(train, datetime:windspeed)
train.svd <- svd(scale(sapply(tr.inputs, unclass)))

gg <- ggplot() + 
    geom_bar(aes(x=1:length(train.svd$d), y=train.svd$d^2/sum(train.svd$d^2)),
             stat="identity",
             fill="dodgerblue") + 
    scale_x_discrete(limits=1:length(tr.inputs)) +
    labs(title="Feature Variance") +
    labs(x="Orthogonal variables") +
    labs(y="Proportion of variance explained")
print(gg)
```

The last eigenvalue (variable 9 above) appears to contribute a negligible amount of information to the input data. The below plot shows the components of this eigenvalue are temperature, 'feels like' temperature, and a little of wind and humidity. This is consistent with our earlier exploration. The low eigenvalue here means that at least one of these 4 variables can be completely neglected. 

```{r pca.atemp, echo=FALSE}
eigval <- 9
gg <- ggplot() + 
    geom_bar(aes(x=1:length(tr.inputs), y=train.svd$v[,eigval]),
             stat="identity",
             fill="dodgerblue") + 
    scale_x_discrete(limits=names(tr.inputs)) +
    labs(title=paste("Composition of eigenvalue",eigval)) +
    labs(x="Feature") +
    labs(y="Scaled Column Means")
print(gg)
```

Finally, exploring the composition of eigenvalue 1 shows that about `r round(train.svd$d[1]*100)`% of the data variation comes from datetime, season, temperature, and a little of windspeed. This means date, time, season, temperature and to some extent wind are intertwined pieces of information, as one might expect. 

```{r pca.season.temp, echo=FALSE}
eigval <- 1
gg <- ggplot() + 
    geom_bar(aes(x=1:length(tr.inputs), y=train.svd$v[,eigval]),
             stat="identity",
             fill="dodgerblue") + 
    scale_x_discrete(limits=names(tr.inputs)) +
    labs(title=paste("Composition of eigenvalue",eigval)) +
    labs(x="Feature") +
    labs(y="Scaled Column Means")
print(gg)
```

Due to the broad spread of orthogonal data across a variety of features, an optimal solution will likely include most of the provided data, with the exception of temp/atemp. These two variables provide very little additional information between each other. 

### Effect of Time of day, weather, and working/non-working days

The below plot shows the effect of time, temperature, weather, and work/nonwork days on daily rental counts. The initial idea for this plot came from [this kaggle script post](https://www.kaggle.com/users/993/ben-hamner/bike-sharing-demand/bike-rentals-by-time-and-temperature) by Ben Hamner, which has been expanded here. 

```{r demand.daily, fig.width=8, fig.height=6}
colors.tempurature <- c("#5e4fa2", "#3288bd", "#66c2a5", "#abdda4", "#e6f598",
                        "#fee08b", "#fdae61", "#f46d43", "#d53e4f", "#9e0142")

gg <- ggplot(train, aes(timeofday, count, color=9/5*temp+32)) +
    facet_grid(workingday ~ weather) +
    geom_point() +
    geom_smooth() +
    theme(plot.title = element_text(size = rel(1.5))) +
    labs(title="Daily Bike Rental Demand \nPer Time of Day, Work/Nonwork day, and Weather") + 
    labs(x="Hour of Day") + 
    labs(y="Bike Rentals Initiated per Hour") +
    scale_colour_gradientn("Temp (°F)", colours=colors.tempurature)
windows()
print(gg)
```

Features to note:

* Sinusoidal shape of demand with respect to hour of the day.
* Weather does not greatly affect the shape of the demand curve with respect to time of day, just the magnitude and density. 
* Weather does seem to effect the average temperature of the data.
* Dependence on demand to atemp, with peak demand in the 80-100°F range. 
* Distinctly differing shape between a work and non-work day, with 
* Peak demand occurs in the afternoon of a warm, nice-weather, work-day.

## Model Development

A random forest model is a convenient method for predicting rental counts given a specified set of variables and is robust to irrelevant or overlapping features. This script is essentially a copy of Ben Hamner's submission for [What drives demand for DC bike rentals?](https://www.kaggle.com/benhamner/bike-sharing-demand/what-drives-demand-for-dc-bike-rentals). 

```{r random.forest}
# features used is modified slightly
train.features <- select(train, season, dayofweek, timeofday, workingday, 
                         weather, temp, humidity, windspeed)

# this is baller Hamner R code, no credit taken here.
train.rf <- randomForest(train.features, train$count, ntree=100, importance=TRUE)

imp <- importance(train.rf, type=1)
featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])

ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +
     geom_bar(stat="identity", fill="#53cfff") +
     coord_flip() + 
     theme_light(base_size=16) +
     xlab("") + 
     ylab("Relative Importance") +
     theme(plot.title   = element_text(size=18),
           strip.text.x = element_blank(),
           axis.text.x  = element_blank(),
           axis.ticks.x = element_blank())
```

Ben also had a wonderful way to visualize the individual feature contributions using scaled partial plots of each feature and their effect on count. Again, not credit taken on my part. 

```{r random.forest.results}
partials <- data.frame()

for (i in seq_along(names(train.features))) {
  partial <- partialPlot(train.rf, sapply(train.features, unclass), 
                         names(train.features)[i], 
                         plot=FALSE)
  xt <- rescale(partial$x)
  partials <- rbind(partials, data.frame(x=partial$x, xt=xt, y=partial$y, 
                                         feature=names(train.features)[i]))
}

ranges <- ddply(partials, "feature", function(d) {
  r <- range(d$y)
  data.frame(feature=d$feature[1], range=r[2]-r[1])
})

features_to_plot <- ranges[ranges$range>0.05*max(ranges$range),"feature"]

ggplot(partials[partials$feature %in% features_to_plot,], 
       aes(x=xt, y=y, color=feature)) +
  geom_line(size=2) +
  theme_light(base_size=16) +
  xlab("Feature Range (Min to Max)") +
  ylab("Hourly Bike Rentals") 
```

```{r export.prediction}
# TODO: move select call to separate extract_features function
prediction <- predict(train.rf, select(test, season, dayofweek, timeofday, workingday, weather, temp, humidity, windspeed))

export.df <- data.frame(datetime=test$datetime,count=prediction)

write.csv(export.df,file="data/Submission2.csv", quote=FALSE, row.names=FALSE)

```

## Future Work

Items remaining for future work:

* Include wind and humidity in the data exploration. 
* Explore alternative prediction techniques.
* Determine if data set is applicable to the DC weather patterns. Data appears to be from much more temperate climate which could greatly affect weather-based behavior. Theory: residents get accustomed to their "average" weather condition, adjusting outdoor activity accordingly. 